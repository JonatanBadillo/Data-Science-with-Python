{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a7dea8",
   "metadata": {},
   "source": [
    "<h2><font color=\"#004D7F\" size=6>Module 3. Data Preprocessing</font></h2>\n",
    "\n",
    "<h1><font color=\"#004D7F\" size=5>3. Resampling Methods</font></h1>\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca53c9f",
   "metadata": {},
   "source": [
    "<h2><font color=\"#004D7F\" size=5>Index</font></h2>\n",
    "<a id=\"indice\"></a>\n",
    "\n",
    "* [1. Introduction](#section1)\n",
    "    * [1.1. Libraries and CSV](#section11)\n",
    "* [2. Cross-Validation](#section2)\n",
    "    * [2.1. _k_-fold Cross Validation](#section21)\n",
    "    * [2.2. Repeated Cross-Validation](#section22)\n",
    "    * [2.3. Leave One Out Cross-Validation](#section23)\n",
    "* [3. Percentage Split](#section3)\n",
    "    * [3.1. Train/Test Percentage Split](#section31)\n",
    "    * [3.2. Random Repeated Train/Test Split](#section32)\n",
    "* [4. Choosing a Technique](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab6490",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\">1. Introduction</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035cd09",
   "metadata": {},
   "source": [
    "The evaluation is an estimate that we can use to talk about how well we believe the algorithm can actually perform in practice. It's not a guarantee of performance. Once we estimate the performance of our algorithm, we can retrain the final algorithm on the entire training dataset and prepare it for operational use. Below, we'll look at four different techniques we can use to split our training dataset and create useful performance estimates for our Machine Learning algorithms:\n",
    "\n",
    "    * How to split a dataset into subsets by percentage for training/validation.\n",
    "    * How to assess model robustness using cross-validation, k-fold, with and without repetitions.\n",
    "    * How to assess model robustness using leave-one-out cross-validation (LOOCV).\n",
    "    * Random repeated train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917de5d6",
   "metadata": {},
   "source": [
    "<a id=\"section11\"></a>\n",
    "## <font color=\"#004D7F\">1.1. Libraries and CSV</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f561b7b",
   "metadata": {},
   "source": [
    "As always, we load the CSV file that we are going to use. Additionally, we will load the main libraries that we will use in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695c63db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.  148.   72.  ...  33.6 627.   50. ]\n",
      " [  1.   85.   66.  ...  26.6 351.   31. ]\n",
      " [  8.  183.   64.  ...  23.3 672.   32. ]\n",
      " ...\n",
      " [  5.  121.   72.  ...  26.2 245.   30. ]\n",
      " [  1.  126.   60.  ...  30.1 349.   47. ]\n",
      " [  1.   93.   70.  ...  30.4 315.   23. ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = pd.read_csv(filename, names = names)\n",
    "array = data.values\n",
    "\n",
    "X = array[ : , 0:8] # All the characteristics of all the rows from the column 0 to 8\n",
    "Y = array[ : , 8] # The target, all the rows in the last column\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed01d29",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\">2. Cross-Validation</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336222e",
   "metadata": {},
   "source": [
    "Cross-validation is a process in which the dataset is divided into $K$ partitions or folds, and $K$ different evaluations are performed, so that all cases are in the test set at least once. Basically, in evaluation $i$, partition $i$ is the test cases, and the rest are the training cases. Finally, the results obtained in the different evaluations are averaged. The following image shows an example of this:\n",
    "<img src=\"https://static.oschina.net/uploads/img/201609/26155106_OfXx.png\" alt=\"cross-validation\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0c615",
   "metadata": {},
   "source": [
    "<a id=\"section21\"></a>\n",
    "## <font color=\"#004D7F\">2.1. k-fold Cross Validation</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbef4e4",
   "metadata": {},
   "source": [
    "The k-fold cross-validation method involves splitting the dataset into k partitions, also called folds. Each subset is held out while the model is trained on all other partitions. This process is repeated until accuracy is determined for each instance in the dataset, providing an overall accuracy estimate. It is a robust method for estimating accuracy, and the size of k can adjust the amount of bias in the estimate, with popular values set at 5 and 10.\n",
    "\n",
    "You can see that we report both the mean and the standard deviation of the performance measure. When summarizing performance measures, it's good practice to summarize the distribution of the measures, in this case assuming a Gaussian distribution of performance (a very reasonable assumption), and reporting the standard deviation and mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fc139",
   "metadata": {},
   "source": [
    "The _k_-fold cross-validation method involves splitting the dataset into _k_ partitions, also called folds. Each subset is held out while the model is trained on all other partitions. This process is repeated until accuracy is determined for each instance in the dataset, providing an overall accuracy estimate. It is a robust method for estimating accuracy, and the size of _k_ can adjust the amount of bias in the estimate, with popular values set at 5 and 10.\n",
    "\n",
    "You can see that we report both the mean and the standard deviation of the performance measure. When summarizing performance measures, it's good practice to summarize the distribution of the measures, in this case assuming a Gaussian distribution of performance (a very reasonable assumption), and reporting the standard deviation and mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6890dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.34% (4.90%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(f\"Accuracy: {results.mean()*100.0:.2f}% ({results.std()*100.0:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6dfda2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
